# # -*- coding: utf-8 -*-
# """Untitled40.ipynb

# Automatically generated by Colab.

# Original file is located at
#     https://colab.research.google.com/drive/1Xy3ZDBgjJTfBMXpLIsK76u9-KhV898BI
# """

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import re
import nltk
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
from sklearn.model_selection import train_test_split
import warnings
nltk.download('punkt', quiet=True)
nltk.download('punkt_tab')
nltk.download('stopwords', quiet=True)
nltk.download('wordnet', quiet=True)
nltk.download('omw-1.4', quiet=True)

warnings.filterwarnings("ignore")

credits = pd.read_csv("/content/tmdb_5000_credits.csv.zip")
movies = pd.read_csv("/content/tmdb_5000_movies.csv.zip")

movies = movies.merge(credits,on="title")

movies.shape

movies.info()

table_dtype_info = pd.DataFrame(movies.dtypes).rename(columns={0:"Column DataType"})
table_null_info = pd.DataFrame(movies.isnull().sum()).rename(columns={0:"Null Value Count"})

table_info_final = pd.concat([table_dtype_info,table_null_info],axis=1)

table_info_final

movies["release_date"] = pd.to_datetime(movies["release_date"])
movies["release_date"] = movies["release_date"].fillna(movies["release_date"].mode()[0])
movies["runtime"] = movies["runtime"].fillna(movies["runtime"].mean())

movies.drop(columns={"tagline","homepage","id","movie_id","spoken_languages"},axis=1,inplace=True)

plt.figure(figsize=(10,5))
plt.title("HeatMap")
sns.heatmap(movies.select_dtypes(exclude="object").corr(),annot=True,cmap='coolwarm')
plt.show()

plt.figure(figsize=(10,5))
plt.title("Movie Status")
plt.bar(movies["status"].unique(),movies["status"].value_counts())
plt.show()

years = movies["release_date"].dt.year
dates_Arr = [
    ((years > 1900) & (years < 2000)).sum(),
    ((years <= 1900) | (years >= 2000)).sum()
]
plt.figure(figsize=(10,5))
plt.title("% of films released per Decade")
plt.pie(dates_Arr,labels=["1900's","2000's"],autopct="%1.1f%%")
plt.show()

import ast
def genres_preprocessing(value):
    name = []
    value = ast.literal_eval(value)
    for val in value:
        name.append(val["name"])
    return " ".join(name)

def cast_preprocessing(value):
    name = []
    value = ast.literal_eval(value)
    for val in value:
        name.append(val["name"])
        name.append(val["character"])
    return " ".join(name)

movies["genres"] = movies["genres"].apply(genres_preprocessing)
movies["keywords"] = movies["keywords"].apply(genres_preprocessing)
movies["production_companies"] = movies["production_companies"].apply(genres_preprocessing)
movies["production_countries"] = movies["production_countries"].apply(genres_preprocessing)
movies["cast"] = movies["cast"].apply(cast_preprocessing)
movies["crew"] = movies["crew"].apply(genres_preprocessing)

movies["information"] = movies["budget"].astype(str) + movies["genres"] + movies["keywords"]+movies["original_language"]+movies["overview"]+movies["popularity"].astype(str) + movies["production_companies"] + movies["production_countries"] + movies["release_date"].astype(str) + movies["revenue"].astype(str) + movies["runtime"].astype(str)+movies["status"] + movies["vote_average"].astype(str) + movies["vote_count"].astype(str)+movies["cast"]+movies["crew"]

movies = movies[["title","information"]]

movies

lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))

def text_preprocessing(text):
    if not isinstance(text, str):
        return ""

    text = text.lower()
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    tokens = word_tokenize(text)
    tokens = [w for w in tokens if w not in stop_words and len(w) > 2]
    tokens = [lemmatizer.lemmatize(w) for w in tokens]

    return " ".join(tokens)
movies["information"] = movies["information"].apply(text_preprocessing)

X,y = movies["information"],movies["title"]
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(X)
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.6,random_state=42)

from sklearn.metrics.pairwise import cosine_similarity
def recommend_similar(movie_title):
    idx_list = movies.index[movies["title"].str.lower() == movie_title.lower()].tolist()
    if not idx_list:
        return f"Title not found: {movie_title}"
    idx = idx_list[0]
    sims = cosine_similarity(X[idx], X).flatten()
    similar_idx = np.argsort(sims)[::-1]
    similar_idx = [i for i in similar_idx if i != idx][:10]

    return movies.loc[similar_idx, ["title"]].assign(similarity=sims[similar_idx])

recommend_similar("batman")